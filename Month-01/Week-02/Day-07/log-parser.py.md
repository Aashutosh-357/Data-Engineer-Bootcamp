# üìä Log Parser Implementation

## üéØ **Task Overview**
Create `log_parser.py` to parse server logs, extract log levels and timestamps, then generate a summary report.

> üí° **Why This Matters:** Log parsing is essential for monitoring, debugging, and analyzing system behavior in production environments.

---

## üì• **Input Format**
```
INFO: User logged in at 2023-01-15 10:30:00
WARN: Disk space low.
ERROR: Database connection failed at 2023-01-15 10:35:22
INFO: Backup completed at 2023-01-15 11:00:00
```

---

# üíª **Basic Implementation**

## üìÑ **log_parser.py**
```python
import re
from collections import defaultdict

def parse_logs(log_string):
    """
    Parse server logs and extract log levels and timestamps
    
    Args:
        log_string: Multi-line string containing server logs
    
    Returns:
        dict: Summary of parsed log data
    """
    lines = log_string.strip().split('\n')
    
    # Counters for summary
    level_counts = defaultdict(int)
    timestamps = []
    
    # Regex patterns
    level_pattern = r'^(INFO|WARN|ERROR|DEBUG)'
    timestamp_pattern = r'\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}'
    
    print("üìä LOG PARSING RESULTS")
    print("=" * 30)
    
    for i, line in enumerate(lines, 1):
        if not line.strip():
            continue
            
        # Extract log level
        level_match = re.search(level_pattern, line)
        level = level_match.group(1) if level_match else 'UNKNOWN'
        
        # Extract timestamp
        timestamp_match = re.search(timestamp_pattern, line)
        timestamp = timestamp_match.group() if timestamp_match else None
        
        # Update counters
        level_counts[level] += 1
        if timestamp:
            timestamps.append(timestamp)
        
        # Print parsed line
        timestamp_str = f" | Time: {timestamp}" if timestamp else ""
        print(f"{i:2d}. Level: {level:5s}{timestamp_str}")
    
    # Print summary
    print("\nüìà SUMMARY")
    print("-" * 20)
    print(f"Total lines: {len([l for l in lines if l.strip()])}")
    
    for level, count in sorted(level_counts.items()):
        print(f"{level}: {count}")
    
    print(f"\nTimestamps found: {len(timestamps)}")
    if timestamps:
        print(f"First: {min(timestamps)}")
        print(f"Last:  {max(timestamps)}")
    
    return {
        'level_counts': dict(level_counts),
        'timestamps': timestamps,
        'total_lines': len([l for l in lines if l.strip()])
    }

def main():
    """Main function with sample log data"""
    
    sample_logs = """INFO: User logged in at 2023-01-15 10:30:00
WARN: Disk space low.
ERROR: Database connection failed at 2023-01-15 10:35:22
INFO: Backup completed at 2023-01-15 11:00:00
DEBUG: Cache cleared at 2023-01-15 11:15:30
ERROR: Authentication failed
INFO: System startup at 2023-01-15 09:00:00
WARN: Memory usage high at 2023-01-15 10:45:15"""
    
    result = parse_logs(sample_logs)
    return result

if __name__ == "__main__":
    main()
```

---

## üöÄ **Expected Output**
```
üìä LOG PARSING RESULTS
==============================
 1. Level: INFO  | Time: 2023-01-15 10:30:00
 2. Level: WARN 
 3. Level: ERROR | Time: 2023-01-15 10:35:22
 4. Level: INFO  | Time: 2023-01-15 11:00:00
 5. Level: DEBUG | Time: 2023-01-15 11:15:30
 6. Level: ERROR
 7. Level: INFO  | Time: 2023-01-15 09:00:00
 8. Level: WARN  | Time: 2023-01-15 10:45:15

üìà SUMMARY
--------------------
Total lines: 8
DEBUG: 1
ERROR: 2
INFO: 3
WARN: 2

Timestamps found: 6
First: 2023-01-15 09:00:00
Last:  2023-01-15 11:15:30
```

---

# üèóÔ∏è **Enhanced Implementation**

## üéØ **Advanced Features**
- **Object-oriented design** with dataclasses
- **Type hints** for better code clarity
- **Modular architecture** for scalability
- **Performance optimizations** with compiled regex

### üìä **Enhanced log_parser.py**
```python
import re
from collections import defaultdict
from dataclasses import dataclass
from typing import Dict, List, Optional, Any

@dataclass(frozen=True)
class LogEntry:
    """Immutable record representing a single parsed log line"""
    line_number: int
    level: str
    timestamp: Optional[str]
    raw_content: str

class LogParser:
    """
    Advanced log parsing engine with structured data extraction
    """
    
    # Pre-compiled regex patterns for performance
    LEVEL_PATTERN = re.compile(r'^(INFO|WARN|ERROR|DEBUG|FATAL|TRACE)')
    TIMESTAMP_PATTERN = re.compile(r'\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}')

    def __init__(self, raw_logs: str):
        self.raw_logs = raw_logs
        self.parsed_entries: List[LogEntry] = []
        self._level_counts: Dict[str, int] = defaultdict(int)

    def _extract_metadata(self, line: str, index: int) -> LogEntry:
        """Extract structured data from a single log line"""
        clean_line = line.strip()
        
        # Extract log level
        level_match = self.LEVEL_PATTERN.search(clean_line)
        level = level_match.group(1) if level_match else 'UNKNOWN'
        
        # Extract timestamp
        time_match = self.TIMESTAMP_PATTERN.search(clean_line)
        timestamp = time_match.group() if time_match else None
        
        return LogEntry(
            line_number=index,
            level=level,
            timestamp=timestamp,
            raw_content=clean_line
        )

    def process(self) -> List[LogEntry]:
        """Execute the parsing pipeline"""
        lines = self.raw_logs.strip().split('\n')
        
        for i, line in enumerate(lines, 1):
            if not line.strip():
                continue
            
            entry = self._extract_metadata(line, i)
            self.parsed_entries.append(entry)
            self._level_counts[entry.level] += 1
            
        return self.parsed_entries

    def get_summary(self) -> Dict[str, Any]:
        """Calculate comprehensive metrics from parsed data"""
        timestamps = [e.timestamp for e in self.parsed_entries if e.timestamp]
        
        return {
            'level_counts': dict(sorted(self._level_counts.items())),
            'total_entries': len(self.parsed_entries),
            'timestamp_range': (min(timestamps), max(timestamps)) if timestamps else (None, None),
            'timestamp_count': len(timestamps),
            'coverage_percentage': round(len(timestamps) / len(self.parsed_entries) * 100, 1) if self.parsed_entries else 0
        }

class LogReporter:
    """Handles presentation and reporting of log analysis results"""
    
    @staticmethod
    def print_detailed_report(entries: List[LogEntry], summary: Dict[str, Any]):
        """Generate comprehensive console report"""
        print("\nüìä DETAILED LOG ANALYSIS")
        print("=" * 45)
        
        for entry in entries:
            time_str = f" | {entry.timestamp}" if entry.timestamp else " | NO TIMESTAMP"
            print(f"L{entry.line_number:02d}: [{entry.level:7s}]{time_str}")

        print("\nüìà STATISTICAL SUMMARY")
        print("-" * 30)
        print(f"Total Entries: {summary['total_entries']}")
        print(f"Timestamp Coverage: {summary['coverage_percentage']}%")
        
        print("\nüìä Level Distribution:")
        for level, count in summary['level_counts'].items():
            percentage = round(count / summary['total_entries'] * 100, 1)
            print(f"  {level:8s}: {count:2d} ({percentage:4.1f}%)")
        
        if summary['timestamp_range'][0]:
            print(f"\n‚è∞ Time Window:")
            print(f"  Start: {summary['timestamp_range'][0]}")
            print(f"  End:   {summary['timestamp_range'][1]}")

    @staticmethod
    def generate_alerts(summary: Dict[str, Any]) -> List[str]:
        """Generate alerts based on log analysis"""
        alerts = []
        
        # Check for high error rates
        error_count = summary['level_counts'].get('ERROR', 0)
        total = summary['total_entries']
        
        if error_count > 0:
            error_rate = error_count / total * 100
            if error_rate > 20:
                alerts.append(f"üö® HIGH ERROR RATE: {error_rate:.1f}% ({error_count}/{total})")
            elif error_rate > 10:
                alerts.append(f"‚ö†Ô∏è  ELEVATED ERRORS: {error_rate:.1f}% ({error_count}/{total})")
        
        # Check timestamp coverage
        if summary['coverage_percentage'] < 50:
            alerts.append(f"üìÖ LOW TIMESTAMP COVERAGE: {summary['coverage_percentage']}%")
        
        return alerts

def main():
    """Enhanced main function with comprehensive analysis"""
    
    sample_logs = """INFO: User logged in at 2023-01-15 10:30:00
WARN: Disk space low.
ERROR: Database connection failed at 2023-01-15 10:35:22
INFO: Backup completed at 2023-01-15 11:00:00
DEBUG: Cache cleared at 2023-01-15 11:15:30
ERROR: Authentication failed
INFO: System startup at 2023-01-15 09:00:00
WARN: Memory usage high at 2023-01-15 10:45:15
FATAL: System crash detected at 2023-01-15 11:30:00
TRACE: Function entry: process_request"""

    try:
        # Initialize parser
        parser = LogParser(sample_logs)
        
        # Process logs
        entries = parser.process()
        summary = parser.get_summary()
        
        # Generate reports
        LogReporter.print_detailed_report(entries, summary)
        
        # Check for alerts
        alerts = LogReporter.generate_alerts(summary)
        if alerts:
            print("\nüö® SYSTEM ALERTS")
            print("-" * 20)
            for alert in alerts:
                print(f"  {alert}")
        else:
            print("\n‚úÖ No critical issues detected")
        
        return summary
        
    except Exception as e:
        print(f"‚ùå Parser Error: {e}")
        return None

if __name__ == "__main__":
    main()
```

---

## üéØ **Enhanced Output**
```
üìä DETAILED LOG ANALYSIS
=============================================
L01: [   INFO] | 2023-01-15 10:30:00
L02: [   WARN] | NO TIMESTAMP
L03: [  ERROR] | 2023-01-15 10:35:22
L04: [   INFO] | 2023-01-15 11:00:00
L05: [  DEBUG] | 2023-01-15 11:15:30
L06: [  ERROR] | NO TIMESTAMP
L07: [   INFO] | 2023-01-15 09:00:00
L08: [   WARN] | 2023-01-15 10:45:15
L09: [  FATAL] | 2023-01-15 11:30:00
L10: [  TRACE] | NO TIMESTAMP

üìà STATISTICAL SUMMARY
------------------------------
Total Entries: 10
Timestamp Coverage: 70.0%

üìä Level Distribution:
   DEBUG:  1 (10.0%)
   ERROR:  2 (20.0%)
   FATAL:  1 (10.0%)
    INFO:  3 (30.0%)
   TRACE:  1 (10.0%)
    WARN:  2 (20.0%)

‚è∞ Time Window:
  Start: 2023-01-15 09:00:00
  End:   2023-01-15 11:30:00

üö® SYSTEM ALERTS
--------------------
  üö® HIGH ERROR RATE: 20.0% (2/10)
```

---

# üéØ **Key Features**

## ‚úÖ **Basic Implementation**
- **Simple function-based** approach
- **Regex pattern matching** for levels and timestamps
- **Basic statistics** and summary reporting
- **Minimal dependencies** (re, collections)

## üöÄ **Enhanced Implementation**
- **Object-oriented design** with dataclasses
- **Type hints** for better code maintainability
- **Performance optimization** with compiled regex
- **Comprehensive reporting** with percentages and alerts
- **Alert system** for anomaly detection
- **Modular architecture** for easy extension

## üíº **Real-World Applications**
- **System monitoring** and health checks
- **Error tracking** and debugging
- **Performance analysis** and optimization
- **Compliance reporting** and auditing
- **Incident response** and troubleshooting

---

*Parse logs, monitor systems, ensure reliability! üìä*